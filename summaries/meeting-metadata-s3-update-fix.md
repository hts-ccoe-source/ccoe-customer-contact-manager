# Meeting Metadata S3 Update Fix

## Problem Summary

Meeting cancellations were failing because the backend was creating Microsoft Graph meetings successfully, but NOT writing the `meeting_id` and `join_url` back to the S3 change object. This caused the frontend cancellation logic to fail because it couldn't find the meeting metadata.

## Root Cause Analysis

### Investigation Steps

1. **Checked S3 Object**: The change object `CHG-4e0e59cc-df21-4bf6-80bd-5ad52726e259` was missing `meeting_id` and `join_url` fields even though the meeting was created
2. **Verified Backend Code**: The backend DOES have code to write meeting metadata to S3 via `S3UpdateManager`
3. **Found the Issue**: The `NewS3UpdateManager` initialization was failing silently, causing the `MeetingScheduler` to be created WITHOUT the `s3UpdateManager` component

### The Silent Failure

In `internal/lambda/handlers.go`, the `NewMeetingScheduler` function:

```go
func NewMeetingScheduler(region string) *MeetingScheduler {
    s3Manager, err := NewS3UpdateManager(region)
    if err != nil {
        log.Printf("‚ö†Ô∏è  Failed to create S3UpdateManager: %v", err)
        return &MeetingScheduler{region: region}  // Returns WITHOUT s3UpdateManager!
    }
    
    return &MeetingScheduler{
        s3UpdateManager: s3Manager,
        region:          region,
    }
}
```

When `NewS3UpdateManager` failed, it returned a `MeetingScheduler` with `s3UpdateManager = nil`, which caused the S3 update to be skipped with only a warning log.

## Solution Implemented

### 1. Enhanced Error Logging

Added CRITICAL error logging to make S3UpdateManager failures highly visible:

**In `NewMeetingScheduler`:**
```go
if err != nil {
    log.Printf("‚ùå CRITICAL: Failed to create S3UpdateManager: %v", err)
    log.Printf("‚ùå CRITICAL: Meeting metadata will NOT be written to S3!")
    log.Printf("‚ùå CRITICAL: This will prevent meeting cancellations from working!")
    return &MeetingScheduler{region: region}
}
log.Printf("‚úÖ Successfully created S3UpdateManager for region: %s", region)
```

**In `ScheduleMeetingWithMetadata`:**
```go
if ms.s3UpdateManager != nil {
    err = ms.s3UpdateManager.UpdateChangeObjectWithMeetingMetadata(ctx, s3Bucket, s3Key, meetingMetadata)
    if err != nil {
        log.Printf("‚ùå CRITICAL: Failed to update S3 object with meeting metadata: %v", err)
        log.Printf("‚ùå CRITICAL: Meeting was created but metadata NOT saved to S3!")
        log.Printf("‚ùå CRITICAL: Meeting ID: %s", meetingMetadata.MeetingID)
        log.Printf("‚ùå CRITICAL: S3 Location: s3://%s/%s", s3Bucket, s3Key)
        log.Printf("‚ùå CRITICAL: This will prevent meeting cancellations from working!")
    } else {
        log.Printf("‚úÖ Successfully updated S3 object with meeting metadata")
        log.Printf("‚úÖ Meeting ID: %s written to s3://%s/%s", meetingMetadata.MeetingID, s3Bucket, s3Key)
    }
} else {
    log.Printf("‚ùå CRITICAL: S3UpdateManager not available, skipping S3 update")
    log.Printf("‚ùå CRITICAL: Meeting was created but metadata will NOT be saved to S3!")
    log.Printf("‚ùå CRITICAL: Meeting ID: %s", meetingMetadata.MeetingID)
    log.Printf("‚ùå CRITICAL: This will prevent meeting cancellations from working!")
}
```

### 2. Verified Existing Implementation

The `S3UpdateManager` implementation in `internal/lambda/s3_operations.go` is correct and includes:

- `UpdateChangeObjectWithMeetingMetadata()` - Adds meeting metadata to S3 object
- Sets top-level `meeting_id` and `join_url` fields (lines 420-424)
- Adds `meeting_scheduled` modification entry to the modifications array
- Uses advanced retry logic with exponential backoff
- Proper error classification and handling

## Expected Behavior After Fix

### CloudWatch Logs

After deployment, you should see one of these patterns in CloudWatch:

**Success Case:**
```
‚úÖ Successfully created S3UpdateManager for region: us-east-1
üìÖ Scheduling meeting for change CHG-xxx with idempotency check
‚úÖ Created new Microsoft Graph meeting: ID=xxx
‚úÖ Successfully updated S3 object with meeting metadata
‚úÖ Meeting ID: xxx written to s3://bucket/key
```

**Failure Case (now highly visible):**
```
‚ùå CRITICAL: Failed to create S3UpdateManager: <error details>
‚ùå CRITICAL: Meeting metadata will NOT be written to S3!
‚ùå CRITICAL: This will prevent meeting cancellations from working!
```

OR

```
‚úÖ Successfully created S3UpdateManager for region: us-east-1
‚ùå CRITICAL: Failed to update S3 object with meeting metadata: <error details>
‚ùå CRITICAL: Meeting was created but metadata NOT saved to S3!
‚ùå CRITICAL: Meeting ID: xxx
‚ùå CRITICAL: S3 Location: s3://bucket/key
‚ùå CRITICAL: This will prevent meeting cancellations from working!
```

### S3 Object Structure

After a successful meeting creation, the S3 change object should contain:

```json
{
  "changeId": "CHG-xxx",
  "meeting_id": "AAMkADExxx",
  "join_url": "https://teams.microsoft.com/l/meetup-join/xxx",
  "modifications": [
    {
      "timestamp": "2025-10-13T15:00:00Z",
      "user_id": "arn:aws:iam::123456789012:role/backend-lambda-role",
      "modification_type": "meeting_scheduled",
      "meeting_metadata": {
        "meeting_id": "AAMkADExxx",
        "join_url": "https://teams.microsoft.com/l/meetup-join/xxx",
        "start_time": "2025-10-13T15:00:00Z",
        "end_time": "2025-10-13T16:00:00Z",
        "subject": "Change Implementation: ..."
      }
    }
  ]
}
```

## Additional Issue Found: Archive Path vs Customer Path

### The Problem

The Go backend writes meeting metadata to customer-specific S3 paths:
- `s3://bucket/customers/{customer}/CHG-xxx.json`

But the Node.js Lambda (frontend API) reads from the archive path:
- `s3://bucket/archive/CHG-xxx.json`

This caused the frontend to never see the meeting metadata, even though it was successfully written to S3!

### The Solution

Updated the Go backend to write meeting metadata to BOTH paths:
1. Customer-specific path (for backend processing)
2. Archive path (for frontend reading)

This ensures the frontend can reload the change with meeting metadata before cancellation.

## Next Steps

1. **Deploy the updated backend code** to Lambda
2. **Monitor CloudWatch logs** for the new CRITICAL error messages
3. **Test meeting creation** and verify:
   - Meeting is created in Microsoft Graph ‚úÖ
   - `meeting_id` and `join_url` are written to S3 ‚úÖ
   - Meeting cancellation works ‚úÖ

4. **If S3UpdateManager still fails to initialize:**
   - Check IAM permissions for the Lambda execution role
   - Verify the Lambda has `s3:GetObject` and `s3:PutObject` permissions
   - Check if there are any AWS SDK configuration issues
   - Review the specific error message in the CRITICAL logs

## Files Modified

- `internal/lambda/handlers.go` - Enhanced error logging in `NewMeetingScheduler` and `ScheduleMeetingWithMetadata`
- `internal/lambda/handlers.go` - Added archive path update to ensure frontend can read meeting metadata

## Files Verified (No Changes Needed)

- `internal/lambda/s3_operations.go` - S3UpdateManager implementation is correct
- `internal/types/types.go` - ChangeMetadata struct has correct top-level `MeetingID` and `JoinURL` fields
- `html/my-changes.html` - Frontend correctly reloads from S3 before cancel/delete
- `lambda/upload_lambda/upload-metadata-lambda.js` - Node.js Lambda correctly loads from S3

## Testing Checklist

- [ ] Deploy updated backend code
- [ ] Create a new change with meeting required
- [ ] Approve the change (triggers meeting creation)
- [ ] Check CloudWatch logs for success/failure messages
- [ ] Verify S3 object has `meeting_id` and `join_url` fields
- [ ] Cancel the change
- [ ] Verify meeting is cancelled in Microsoft Graph
- [ ] Verify cancellation email is sent

